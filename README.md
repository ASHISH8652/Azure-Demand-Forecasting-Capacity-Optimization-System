# ğŸš€ Azure Demand Forecasting & Capacity Optimization System
## ğŸ“Š End-to-End Azure Capacity Optimization  & Demand Forecasting Project

An end-to-end data engineering and machine learning project designed to analyze, clean, validate, and prepare Azure cloud demand data for intelligent forecasting and capacity planning.
--
This project simulates a real-world cloud infrastructure analytics pipeline, following professional ML workflow practices across four milestones.
--

## ğŸŒ Overview

> Cloud service providers must forecast infrastructure demand accurately to:
> Prevent over-provisioning (wasted cost)
> Prevent under-provisioning (service outages)
> Maintain high availability
> Optimize operational efficiency
> This project builds a structured system to:
---
## ğŸŒŸ Key Features (Implemented)
âœ” Clean real-world noisy cloud data

âœ” Validate business constraints

âœ” Analyze demand patterns

âœ” Prepare data for forecasting models

âœ” Eventually deploy a forecasting pipeline

---
---
## ğŸ—ï¸ Project Architecture (Milestone-Based Development)
```
Azure-Demand-Forecasting-Capacity-Optimization-System/
â”‚
â”œâ”€â”€ milestone1_data_cleaning/
â”‚   â”œâ”€â”€ notebook.ipynb
â”‚   â”œâ”€â”€ cleaned_dataset.csv
â”‚
â”œâ”€â”€ milestone2_feature_engineering/
â”‚   â”œâ”€â”€ Milestone2_Feature_Engineering.ipynb
â”‚   â”œâ”€â”€ feature_engineered_dataset.csv
â”‚
â”œâ”€â”€ milestone3_model_training/
â”‚
â”œâ”€â”€ milestone4_deployment/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_dataset.csv
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```
---
## ğŸ§¹ Milestone 1 â€“ Data Cleaning & Exploratory Data Analysis

* Milestone 1 focuses on transforming a noisy bi-weekly Azure dataset into a validated, production-ready dataset.
---
## ğŸ” Dataset Features
* Column	Description 
* time_stamp	Bi-weekly usage date
* region	Azure deployment region
* service_type	Compute / Storage
* usage_units	Actual demand
* provisioned_capacity	Allocated capacity
* cost_usd	Usage cost
* availability_pct	Service uptime percentage
* âš™ï¸ Data Quality Issues Handled
---
### The raw dataset intentionally contained:

* Missing values (~5%)
* Duplicate records (~3%)
* Inconsistent region formatting
* Cost rounding inconsistencies
* Business rule violations
  
### ğŸ›  Cleaning Steps Implemented

âœ” Duplicate removal
âœ” Missing value imputation
âœ” Datetime conversion
âœ” Region standardization
âœ” Time-series interpolation
âœ” Cost rounding correction
âœ” Business validation rules
âœ” Final dataset verification

### ğŸ“ˆ Exploratory Data Analysis

## Milestone 1 includes:

ğŸ“Š Overall demand trend over time
ğŸ“Š Region-wise average demand
ğŸ“Š Service-type specific demand trend
ğŸ“Š Statistical summary validation

## ğŸ§  Technologies Used

* **Python 3.9+**
* **Pandas**
* **NumPy**
* **Matplotlib**
* **Jupyter Notebook**

## ğŸ“Š Business Validation Logic

### To ensure real-world correctness:
* Usage must not exceed provisioned capacity
* Availability must remain between 90% â€“ 100%
* Cost values standardized to 2 decimal precision
* Time-series data properly formatted

## ğŸ”§ Milestone 2 â€“ Feature Engineering & Data Wrangling

### ğŸ”¬ Milestone 2 â€“ Feature Engineering & Data Wrangling

> Milestone 2 transforms the cleaned dataset into a model-ready forecasting dataset by enriching it with time-series intelligence and business-driven derived features.

## ğŸ¯ Objective
* Prepare the dataset for forecasting models through:
* Identification of demand-driving variables
* Creation of lag-based historical influence features
* Detection of abnormal usage spikes
* Engineering rolling statistics for trend smoothing
* Structuring consistent time-series schema

## ğŸ§  Feature Engineering Implemented
### ğŸ”¹ Time-Based Features
* Year
* Month
* Quarter
* Week of Year
* Month Start / End Flags

> These allow models to understand seasonal demand behavior.

### ğŸ”¹ Lag Features
* lag_1
* lag_2
* lag_4
* lag_8

> These capture historical demand memory across region + service combinations.

### ğŸ”¹ Rolling Statistics
* rolling_mean_3
* rolling_mean_6
* rolling_std_3
* rolling_std_6

> These smooth short-term fluctuations and measure volatility.

### ğŸ”¹ Business Context Features
* Capacity Utilization (usage / provisioned_capacity)
* Growth Rate (short-term & medium-term)
* Demand Spike Flag (statistical anomaly detection)

> These features connect technical modeling with business impact.

### ğŸ”¹ Data Wrangling Steps

âœ” Time sorting per region + service
âœ” Consistent time granularity
âœ” Categorical encoding
âœ” Removal of lag-induced null values
âœ” Final model-ready schema export

### ğŸ“¦ Output
* feature_engineered_dataset.csv

> This dataset is now ready for:
* ARIMA / SARIMA
* Prophet
* XGBoost
* LSTM


## ğŸš€ Upcoming Milestones
ğŸ”¹ Milestone 3 â€“ Model Development

* ARIMA / SARIMA
* LSTM
* Regression baselines
* Model evaluation metrics

ğŸ”¹ Milestone 4 â€“ Deployment

* Streamlit dashboard
* Forecast visualization
* Real-time prediction interface

## ğŸ“Œ Academic & Industry Value

âœ” End-to-end ML pipeline thinking
âœ” Real-world cloud infrastructure use case
âœ” Business validation rules applied
âœ” Clean structured repository
âœ” Scalable system architecture

## ğŸ‘¨â€ğŸ’» Author

Ashish Kumar Prusty
B.Tech â€“ Artificial Intelligence & Machine Learning
GitHub: https://github.com/ASHISH8652

## ğŸ“œ License

This project is licensed under the MIT License.

â€œData is not useful until it is clean, validated, and trusted.â€
